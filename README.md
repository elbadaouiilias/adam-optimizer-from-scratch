# adam-optimizer-from-scratch
Implementation of the Adam optimizer for stochastic gradient-based optimization, as described in the paper "Adam: A Method for Stochastic Optimization" (arXiv:1412.6980). Includes a simple, from-scratch Python version and usage examples.
