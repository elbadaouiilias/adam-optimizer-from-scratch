{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Comparaison des optimiseurs SGD et Adam\n",
    "\n",
    "Ce notebook compare deux algorithmes d'optimisation populaires en Deep Learning :\n",
    "- **SGD** (Stochastic Gradient Descent) avec momentum\n",
    "- **Adam** (Adaptive Moment Estimation)\n",
    "\n",
    "Nous allons les impl√©menter de A √† Z pour bien comprendre leur fonctionnement !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Imports n√©cessaires\n",
    "\n",
    "On importe PyTorch pour cr√©er notre r√©seau de neurones et matplotlib pour visualiser les r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Impl√©mentation de SGD personnalis√©\n",
    "\n",
    "**SGD (Stochastic Gradient Descent)** est l'optimiseur de base :\n",
    "- Il met √† jour les poids en suivant la direction oppos√©e du gradient\n",
    "- Le **momentum** permet d'acc√©l√©rer la convergence en gardant une \"m√©moire\" des directions pr√©c√©dentes\n",
    "\n",
    "**Formule** : `v = momentum √ó v + gradient` puis `poids = poids - learning_rate √ó v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDPerso:\n",
    "    def __init__(self, params, lr=0.01, momentum=0.0):\n",
    "        \"\"\"Initialise l'optimiseur SGD.\n",
    "        \n",
    "        Args:\n",
    "            params: Les param√®tres du mod√®le √† optimiser\n",
    "            lr: Taux d'apprentissage (learning rate)\n",
    "            momentum: Coefficient de momentum (0 = pas de momentum)\n",
    "        \"\"\"\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # v stocke la \"vitesse\" pour chaque param√®tre (utilis√© pour le momentum)\n",
    "        self.v = [torch.zeros_like(p.data) for p in self.params]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Remet √† z√©ro tous les gradients avant le backward.\"\"\"\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Effectue une √©tape d'optimisation en mettant √† jour les poids.\"\"\"\n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = p.grad.data  # R√©cup√®re le gradient\n",
    "            # Mise √† jour de la vitesse avec momentum\n",
    "            self.v[i] = self.momentum * self.v[i] + g\n",
    "            # Mise √† jour des poids\n",
    "            p.data -= self.lr * self.v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Impl√©mentation d'Adam personnalis√©\n",
    "\n",
    "**Adam** est un optimiseur plus sophistiqu√© qui :\n",
    "- Adapte le learning rate pour chaque param√®tre individuellement\n",
    "- Utilise deux moments : **m** (moyenne des gradients) et **v** (variance des gradients)\n",
    "- Corrige le biais d'initialisation avec des facteurs `m_hat` et `v_hat`\n",
    "\n",
    "**Avantages** : Converge souvent plus rapidement et de fa√ßon plus stable que SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamPerso:\n",
    "    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        \"\"\"Initialise l'optimiseur Adam.\n",
    "        \n",
    "        Args:\n",
    "            params: Les param√®tres du mod√®le\n",
    "            lr: Taux d'apprentissage\n",
    "            beta1: Coefficient pour le moment d'ordre 1 (moyenne des gradients)\n",
    "            beta2: Coefficient pour le moment d'ordre 2 (variance des gradients)\n",
    "            eps: Petit terme pour √©viter la division par z√©ro\n",
    "        \"\"\"\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = beta1, beta2\n",
    "        self.eps = eps\n",
    "        self.t = 0  # Compteur d'it√©rations\n",
    "        # m: moyenne des gradients, v: variance des gradients\n",
    "        self.m = [torch.zeros_like(p.data) for p in self.params]\n",
    "        self.v = [torch.zeros_like(p.data) for p in self.params]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Remet √† z√©ro tous les gradients.\"\"\"\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Effectue une √©tape d'optimisation Adam.\"\"\"\n",
    "        self.t += 1  # Incr√©mente le compteur\n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = p.grad.data\n",
    "            # Mise √† jour du moment d'ordre 1 (moyenne mobile exponentielle du gradient)\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "            # Mise √† jour du moment d'ordre 2 (variance)\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g * g)\n",
    "            # Correction du biais (important surtout au d√©but de l'entra√Ænement)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            # Mise √† jour des poids avec un learning rate adaptatif\n",
    "            p.data -= self.lr * m_hat / (torch.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† D√©finition du r√©seau de neurones\n",
    "\n",
    "On cr√©e un **MLP** (Multi-Layer Perceptron) simple avec :\n",
    "- Une couche d'entr√©e\n",
    "- Une couche cach√©e avec activation ReLU\n",
    "- Une couche de sortie\n",
    "\n",
    "C'est un r√©seau basique mais suffisant pour comparer nos optimiseurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, d_out):\n",
    "        \"\"\"Cr√©e un r√©seau √† 2 couches.\n",
    "        \n",
    "        Args:\n",
    "            d_in: Dimension d'entr√©e\n",
    "            d_hid: Nombre de neurones dans la couche cach√©e\n",
    "            d_out: Dimension de sortie\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_hid),  # Couche lin√©aire entr√©e -> cach√©e\n",
    "            nn.ReLU(),               # Fonction d'activation non-lin√©aire\n",
    "            nn.Linear(d_hid, d_out)  # Couche lin√©aire cach√©e -> sortie\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Passe avant dans le r√©seau.\"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Fonction d'entra√Ænement\n",
    "\n",
    "Cette fonction entra√Æne notre mod√®le sur plusieurs √©poques :\n",
    "1. **Forward pass** : Calcul de la pr√©diction\n",
    "2. **Calcul de la loss** : Erreur entre pr√©diction et v√©rit√©\n",
    "3. **Backward pass** : Calcul des gradients\n",
    "4. **Optimisation** : Mise √† jour des poids\n",
    "\n",
    "On utilise la **MSE** (Mean Squared Error) comme fonction de perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrainer(model, optim, X, y, epochs=150):\n",
    "    \"\"\"Entra√Æne le mod√®le et retourne l'historique des pertes.\n",
    "    \n",
    "    Args:\n",
    "        model: Le r√©seau de neurones\n",
    "        optim: L'optimiseur (SGD ou Adam)\n",
    "        X: Donn√©es d'entr√©e\n",
    "        y: Valeurs cibles\n",
    "        epochs: Nombre d'√©poques d'entra√Ænement\n",
    "    \n",
    "    Returns:\n",
    "        Liste des pertes √† chaque √©poque\n",
    "    \"\"\"\n",
    "    loss_fn = nn.MSELoss()  # Fonction de perte MSE\n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        pred = model(X)           # 1. Pr√©diction\n",
    "        loss = loss_fn(pred, y)   # 2. Calcul de l'erreur\n",
    "        optim.zero_grad()         # 3. R√©initialise les gradients\n",
    "        loss.backward()           # 4. Calcule les gradients\n",
    "        optim.step()              # 5. Met √† jour les poids\n",
    "        losses.append(loss.item())  # Sauvegarde la perte\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ G√©n√©ration des donn√©es synth√©tiques\n",
    "\n",
    "On cr√©e un probl√®me de r√©gression lin√©aire simple :\n",
    "- `X` : 800 exemples avec 10 features al√©atoires\n",
    "- `y = X √ó W_true + bruit` : Les vraies valeurs avec un peu de bruit gaussien\n",
    "\n",
    "Le but du r√©seau sera d'apprendre √† pr√©dire `y` √† partir de `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixe le seed pour la reproductibilit√©\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Param√®tres du dataset\n",
    "N, D_in, D_out = 800, 10, 1\n",
    "\n",
    "# G√©n√©ration des donn√©es\n",
    "X = torch.randn(N, D_in)              # Features al√©atoires\n",
    "W_true = torch.randn(D_in, D_out)     # Poids \"vrais\" √† d√©couvrir\n",
    "y = X @ W_true + 0.1 * torch.randn(N, D_out)  # Cibles avec bruit\n",
    "\n",
    "print(f\"üìä Dataset cr√©√© : {N} exemples, {D_in} features\")\n",
    "print(f\"   Shape de X : {X.shape}\")\n",
    "print(f\"   Shape de y : {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ Entra√Ænement avec SGD\n",
    "\n",
    "On entra√Æne un premier mod√®le avec **SGD + momentum** :\n",
    "- Learning rate : 0.01\n",
    "- Momentum : 0.9 (pour acc√©l√©rer la convergence)\n",
    "- 200 √©poques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 32   # Nombre de neurones dans la couche cach√©e\n",
    "epochs = 200  # Nombre d'√©poques\n",
    "\n",
    "# Cr√©ation du mod√®le et de l'optimiseur SGD\n",
    "model_sgd = MLP(D_in, hidden, D_out)\n",
    "opt_sgd = SGDPerso(model_sgd.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"üîÑ Entra√Ænement avec SGD...\")\n",
    "loss_sgd = entrainer(model_sgd, opt_sgd, X, y, epochs)\n",
    "print(f\"‚úÖ Termin√© ! Loss finale : {loss_sgd[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Entra√Ænement avec Adam\n",
    "\n",
    "On entra√Æne un second mod√®le identique mais avec **Adam** :\n",
    "- Learning rate : 0.01 (m√™me que SGD pour comparer)\n",
    "- Param√®tres beta par d√©faut (0.9 et 0.999)\n",
    "- 200 √©poques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'un nouveau mod√®le identique et de l'optimiseur Adam\n",
    "model_adam = MLP(D_in, hidden, D_out)\n",
    "opt_adam = AdamPerso(model_adam.parameters(), lr=0.01)\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"üîÑ Entra√Ænement avec Adam...\")\n",
    "loss_adam = entrainer(model_adam, opt_adam, X, y, epochs)\n",
    "print(f\"‚úÖ Termin√© ! Loss finale : {loss_adam[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualisation des r√©sultats\n",
    "\n",
    "On compare graphiquement les deux optimiseurs :\n",
    "- **Graphique 1** (√©chelle lin√©aire) : Vue g√©n√©rale de l'√©volution de la loss\n",
    "- **Graphique 2** (√©chelle log) : Permet de mieux voir les diff√©rences en fin de convergence\n",
    "\n",
    "**√Ä observer** : la vitesse de convergence et la stabilit√© de chaque m√©thode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Graphique 1 : √âchelle lin√©aire\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_sgd, label=\"SGD\", linewidth=2)\n",
    "plt.plot(loss_adam, label=\"Adam\", linewidth=2)\n",
    "plt.title(\"√âvolution de la loss\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2 : √âchelle logarithmique\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(loss_sgd, label=\"SGD\", linewidth=2)\n",
    "plt.semilogy(loss_adam, label=\"Adam\", linewidth=2)\n",
    "plt.title(\"Convergence (√©chelle log)\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (log)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà R√©sultats finaux et analyse\n",
    "\n",
    "Comparons les performances finales des deux optimiseurs et analysons les r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"üìä R√âSULTATS FINAUX\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Loss finale SGD  : {loss_sgd[-1]:.5f}\")\n",
    "print(f\"Loss finale Adam : {loss_adam[-1]:.5f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßê Analyse et conclusions\n",
    "\n",
    "### Observations cl√©s :\n",
    "\n",
    "**SGD avec momentum :**\n",
    "- ‚úÖ Fonctionne bien et converge correctement\n",
    "- ‚ö†Ô∏è Prend plus de temps √† se stabiliser\n",
    "- üìâ La descente peut √™tre un peu \"nerveuse\"\n",
    "\n",
    "**Adam :**\n",
    "- ‚úÖ Convergence plus fluide, surtout au d√©but\n",
    "- ‚úÖ Atteint une meilleure loss plus rapidement\n",
    "- ‚úÖ Plus stable pendant toute la descente\n",
    "\n",
    "### Conclusion :\n",
    "Dans cette exp√©rience, les deux optimiseurs arrivent √† des r√©sultats finaux proches, mais **Adam** se montre plus efficace :\n",
    "- Il converge plus vite\n",
    "- Il est plus stable\n",
    "- Il n√©cessite moins de tuning des hyperparam√®tres\n",
    "\n",
    "C'est pourquoi Adam est souvent le choix par d√©faut en Deep Learning ! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì Analyse d√©taill√©e :\")\n",
    "print(\"-\" * 50)\n",
    "print(\"SGD :\")\n",
    "print(\"  ‚Ä¢ Marche bien mais met du temps √† stabiliser\")\n",
    "print(\"  ‚Ä¢ Le momentum aide mais reste moins fluide qu'Adam\")\n",
    "print()\n",
    "print(\"Adam :\")\n",
    "print(\"  ‚Ä¢ Plus fluide d√®s le d√©but de l'entra√Ænement\")\n",
    "print(\"  ‚Ä¢ Atteint une meilleure loss plus rapidement\")\n",
    "print(\"  ‚Ä¢ Reste plus stable sur toute la descente\")\n",
    "print()\n",
    "print(\"üí° Conclusion : Les deux finissent proches, mais Adam\")\n",
    "print(\"   converge plus vite et de fa√ßon plus stable !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}